{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48584f77-3c66-4dd3-8aa9-b5280fd80098",
   "metadata": {},
   "source": [
    "## YOLO -v5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7162cc7f-49d7-4d5f-8db7-24224b03913a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\opdar/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "C:\\Users\\opdar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\opdar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "YOLOv5  2024-3-16 Python-3.11.3 torch-2.2.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import time\n",
    "\n",
    "class YoloDetector():\n",
    "    def __init__(self):\n",
    "        # Using yolov5s for our purposes of object detection, you may use a larger model\n",
    "        self.model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
    "        self.classes = self.model.names\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        print('Using Device:', self.device)\n",
    "    \n",
    "    def detect_cars(self, frame):\n",
    "        self.model.to(self.device)\n",
    "        results = self.model(frame)\n",
    "        predictions = results.xyxy[0]\n",
    "\n",
    "        car_boxes = []\n",
    "        for pred in predictions:\n",
    "            if pred[4] > 0.6 and pred[5] == 2:  # Car class index\n",
    "                car_boxes.append(pred[:4].cpu().numpy().astype(int))\n",
    "\n",
    "        return car_boxes\n",
    "\n",
    "# Initialize YOLO detector\n",
    "yolo_detector = YoloDetector()\n",
    "\n",
    "# Open video capture\n",
    "cap = cv2.VideoCapture('highway.mp4')\n",
    "w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n",
    "\n",
    "# Define codec and create VideoWriter object\n",
    "out = cv2.VideoWriter(\"yolo_v5_small_fps.avi\",\n",
    "                       cv2.VideoWriter_fourcc(*'mp4v'),\n",
    "                       fps,\n",
    "                       (w, h))\n",
    "\n",
    "start_time = time.time()\n",
    "frame_count = 0\n",
    "fps_text = \"\"\n",
    "while cap.isOpened():\n",
    "    start_frame_time = time.time()\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Detect cars in the frame\n",
    "    car_boxes = yolo_detector.detect_cars(frame)\n",
    "    \n",
    "    # Draw bounding boxes around cars\n",
    "    for box in car_boxes:\n",
    "        x1, y1, x2, y2 = box\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "    # Calculate FPS\n",
    "    frame_count += 1\n",
    "    if frame_count % 10 == 0:  \n",
    "        end_time = time.time()\n",
    "        fps = frame_count / (end_time - start_time)\n",
    "        fps_text = f\"FPS: {round(fps, 2)}\"\n",
    "        frame_count = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "    # Display FPS on frame\n",
    "    cv2.putText(frame, fps_text, (30, 300), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    \n",
    "    cv2.imshow('Car Detection', frame)\n",
    "    out.write(frame)\n",
    "    \n",
    "    end_frame_time = time.time()\n",
    "    frame_processing_time = end_frame_time - start_frame_time\n",
    "    wait_time_ms = max(int((1 / fps * 1000) - frame_processing_time * 1000), 1)\n",
    "    \n",
    "    if cv2.waitKey(wait_time_ms) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a284d4-29da-4b11-b6d4-99961ae601a7",
   "metadata": {},
   "source": [
    "## YOLOv2-voc,YOLOv2,YOLOv2-tiny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8496230c-0508-451c-acf4-32de831cffb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Layers: ['conv_0', 'bn_0', 'leaky_1', 'pool_1', 'conv_2', 'bn_2', 'leaky_3', 'pool_3', 'conv_4', 'bn_4', 'leaky_5', 'conv_5', 'bn_5', 'leaky_6', 'conv_6', 'bn_6', 'leaky_7', 'pool_7', 'conv_8', 'bn_8', 'leaky_9', 'conv_9', 'bn_9', 'leaky_10', 'conv_10', 'bn_10', 'leaky_11', 'pool_11', 'conv_12', 'bn_12', 'leaky_13', 'conv_13', 'bn_13', 'leaky_14', 'conv_14', 'bn_14', 'leaky_15', 'conv_15', 'bn_15', 'leaky_16', 'conv_16', 'bn_16', 'leaky_17', 'pool_17', 'conv_18', 'bn_18', 'leaky_19', 'conv_19', 'bn_19', 'leaky_20', 'conv_20', 'bn_20', 'leaky_21', 'conv_21', 'bn_21', 'leaky_22', 'conv_22', 'bn_22', 'leaky_23', 'conv_23', 'bn_23', 'leaky_24', 'conv_24', 'bn_24', 'leaky_25', 'identity_25', 'conv_26', 'bn_26', 'leaky_27', 'reorg_27', 'concat_28', 'conv_29', 'bn_29', 'leaky_30', 'conv_30', 'permute_31', 'detection_out']\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load YOLOv2 model\n",
    "net = cv2.dnn.readNet(\"yolov2-voc.weights\", \"yolov2-voc.cfg\")\n",
    "# net = cv2.dnn.readNet(\"yolov2.weights\",\"yolov2.cfg\")\n",
    "# net = cv2.dnn.readNet(\"yolov2-tiny.weights\",\"yolov2-tiny.cfg\")\n",
    "\n",
    "\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i] for i in range(len(layer_names))]\n",
    "print(\"Output Layers:\", output_layers)\n",
    "def detect_objects(frame):\n",
    "    height, width, channels = frame.shape\n",
    "\n",
    "    # Preprocess the image\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    # Process detection outputs\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            if len(scores) > 0:\n",
    "                class_id = np.argmax(scores)\n",
    "                if class_id<20:\n",
    "                    confidence = scores[class_id]\n",
    "                    if confidence > 0.2:  # Class ID for cars in VOC dataset\n",
    "                        # Object detected\n",
    "                        # print(confidence,class_id)\n",
    "                        center_x = int(detection[0] * width)\n",
    "                        center_y = int(detection[1] * height)\n",
    "                        w = int(detection[2] * width)\n",
    "                        h = int(detection[3] * height)\n",
    "                        x = int(center_x - w / 2)\n",
    "                        y = int(center_y - h / 2)\n",
    "                        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "    return frame\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Real-time car tracking\n",
    "cap = cv2.VideoCapture(\"highway.mp4\")  # Replace \"video.mp4\" with your video file or camera index\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = detect_objects(frame)\n",
    "    cv2.imshow(\"Car Tracking\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4861a719-14f4-4cd3-a6e8-0b1d49ab5a40",
   "metadata": {},
   "source": [
    "## Faster RCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6073c7fe-4510-41da-ae5f-560b62d7ca09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total processing time: 21.93 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Load pre-trained Faster R-CNN model from torchvision\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Function to perform object detection using the pre-trained model\n",
    "def detect_objects(frame):\n",
    "    \n",
    "    frame_tensor = torch.from_numpy(frame / 255.0).permute(2, 0, 1).float().unsqueeze(0)\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        predictions = model(frame_tensor)\n",
    "\n",
    "    # Process predictions\n",
    "    boxes = predictions[0]['boxes'].cpu().numpy()\n",
    "    scores = predictions[0]['scores'].cpu().numpy()\n",
    "    labels = predictions[0]['labels'].cpu().numpy()\n",
    "\n",
    "    # Draw bounding boxes on the frame\n",
    "    for box, score, label in zip(boxes, scores, labels):\n",
    "        # print(label)\n",
    "        if score > 0.8 and label==3:  # Adjust confidence threshold as needed\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            # cv2.putText(frame, f'Class: {label}, Score: {score}', (x1, y1 - 10),\n",
    "            #             cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    return frame\n",
    "\n",
    "# Open video capture\n",
    "cap = cv2.VideoCapture('highway.mp4')\n",
    "\n",
    "# Get frame rate of the video\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# frame Skipping\n",
    "frame_skip = int(fps / 10)  \n",
    "frame_count = 0\n",
    "start_time = time.time()\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "    if frame_count % frame_skip != 0:\n",
    "        continue\n",
    "\n",
    "    # Perform object detection on the frame\n",
    "    result_frame = detect_objects(frame)\n",
    "    cv2.imshow('Object Detection', result_frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(f\"Total processing time: {total_time:.2f} seconds\")\n",
    "\n",
    "# Release video capture and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c383c475-3a31-49b3-8ab7-2ff5846d1df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total processing time: 19.77 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "\n",
    "#Initialise the object tracker class\n",
    "object_tracker = DeepSort()\n",
    "# Load pre-trained Faster R-CNN model from torchvision\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Function to perform object detection using the pre-trained model\n",
    "def detect_objects(frame):\n",
    "    frame_tensor = torch.from_numpy(frame / 255.0).permute(2, 0, 1).float().unsqueeze(0)\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        predictions = model(frame_tensor)\n",
    "\n",
    "    # Process predictions\n",
    "    boxes = predictions[0]['boxes'].numpy()\n",
    "    scores = predictions[0]['scores'].numpy()\n",
    "    labels = predictions[0]['labels'].numpy()\n",
    "\n",
    "    return boxes, scores, labels\n",
    "\n",
    "# Open video capture\n",
    "cap = cv2.VideoCapture('highway.mp4')\n",
    "\n",
    "# Get frame rate of the video\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# Process every nth frame to speed up processing\n",
    "frame_skip = int(fps / 10)  # Adjust as needed\n",
    "\n",
    "frame_count = 0\n",
    "start_time = time.time()\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, img = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "    if frame_count % frame_skip != 0:\n",
    "        continue\n",
    "\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    # Perform object detection on the frame\n",
    "    boxes, scores, labels = detect_objects(img)\n",
    "\n",
    "    # Overlay detected bounding boxes on the frame\n",
    "    # for box, score, label in zip(boxes, scores, labels):\n",
    "    #     if score > 0.5 and label == 3:  # Adjust confidence threshold and class as needed\n",
    "    #         x1, y1, x2, y2 = map(int, box)\n",
    "    #         cv2.rectangle(img, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "\n",
    "    # Object tracking part\n",
    "    if img.any() is not None:\n",
    "        detections = [(box, score, label) for box, score, label in zip(boxes, scores, labels) if score > 0.9 and label == 3]\n",
    "\n",
    "        # Convert bounding box data to the format expected by the object tracker\n",
    "        bounding_boxes = [([box[0], box[1], box[2] - box[0], box[3] - box[1]], score, label) for box, score, label in detections]\n",
    "        \n",
    "        # Update tracks using the formatted bounding box data\n",
    "        tracks = object_tracker.update_tracks(bounding_boxes, frame=img)\n",
    "        d = set()\n",
    "        for track in tracks:\n",
    "            if not track.is_confirmed():\n",
    "                continue\n",
    "            track_id = track.track_id\n",
    "            ltrb = track.to_ltrb()\n",
    "\n",
    "            bbox = ltrb\n",
    "\n",
    "            cv2.rectangle(img, (int(bbox[0]),int(bbox[1])),(int(bbox[2]),int(bbox[3])),(0,255,0),2)\n",
    "            if track not in d:\n",
    "                d.add(track_id)\n",
    "            cv2.putText(img, str(len(d)), (int(bbox[0]),int(bbox[1]-10)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,255), 2)\n",
    "\n",
    "    end = time.perf_counter()\n",
    "    totalTime = end - start\n",
    "    fps = 1 / totalTime\n",
    "\n",
    "    cv2.putText(img, f'FPS: {int(fps)}', (20,70), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0,255,0), 2)\n",
    "    img = cv2.resize(img,(1800,900))\n",
    "    cv2.imshow('img', img)\n",
    "\n",
    "    k = cv2.waitKey(1)\n",
    "    if k & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(f\"Total processing time: {total_time:.2f} seconds\")\n",
    "\n",
    "# Release video capture and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6ce4eb-19b4-40bc-a552-012b02065dab",
   "metadata": {},
   "source": [
    "## Faster RCCN  and DeepSORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd7ec782-222c-4735-a314-513d907302fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total processing time: 67.92 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "\n",
    "# Initialise the object tracker class\n",
    "object_tracker = DeepSort()\n",
    "# Load pre-trained Faster R-CNN model from torchvision\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Define the line near the bottom of the frame for counting cars\n",
    "line_y = 800  \n",
    "\n",
    "# Function to check if a point is above a line\n",
    "def above_line(point, line_y):\n",
    "    return point[1] < line_y\n",
    "    \n",
    "# Function to perform object detection using the pre-trained model\n",
    "def detect_objects(frame):\n",
    "    # Convert frame to torch tensor\n",
    "    frame_tensor = torch.from_numpy(frame / 255.0).permute(2, 0, 1).float().unsqueeze(0)\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        predictions = model(frame_tensor)\n",
    "\n",
    "    # Process predictions\n",
    "    boxes = predictions[0]['boxes'].numpy()\n",
    "    scores = predictions[0]['scores'].numpy()\n",
    "    labels = predictions[0]['labels'].numpy()\n",
    "\n",
    "    return boxes, scores, labels\n",
    "\n",
    "# Open video capture\n",
    "cap = cv2.VideoCapture('highway.mp4')\n",
    "\n",
    "# Get frame rate of the video\n",
    "w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n",
    "\n",
    "# Process every nth frame to speed up processing\n",
    "frame_skip = int(fps / 20)  # Adjust as needed\n",
    "\n",
    "# Define codec and create VideoWriter object\n",
    "out = cv2.VideoWriter(\"faster_rcnn_deep_sort.avi\",\n",
    "                       cv2.VideoWriter_fourcc(*'mp4v'),\n",
    "                       fps,\n",
    "                       (w, h))\n",
    "frame_count = 0\n",
    "start_time = time.time()\n",
    "previous_count = 0\n",
    "current_count = 0\n",
    "d = set()\n",
    "d1 = {}\n",
    "sort_paths = {}\n",
    "j = 1\n",
    "while cap.isOpened():\n",
    "    ret, img = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "    if frame_count % frame_skip != 0:\n",
    "        continue\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    cv2.line(img,(70,750),(1100,750),(255,255,255),2)\n",
    "\n",
    "    # Perform object detection on the frame\n",
    "    boxes, scores, labels = detect_objects(img)\n",
    "\n",
    "    # Overlay detected bounding boxes on the frame\n",
    "    if img.any() is not None:\n",
    "        detections = [(box, score, label) for box, score, label in zip(boxes, scores, labels) if score > 0.85 and label == 3]\n",
    "\n",
    "        # Convert bounding box data to the format expected by the object tracker\n",
    "        bounding_boxes = [([box[0], box[1], box[2] - box[0], box[3] - box[1]], score, label) for box, score, label in detections]\n",
    "        \n",
    "        # Update tracks using the formatted bounding box data\n",
    "        tracks = object_tracker.update_tracks(bounding_boxes, frame=img)\n",
    "        for track in tracks:\n",
    "            if not track.is_confirmed():\n",
    "                continue\n",
    "            track_id = track.track_id\n",
    "            ltrb = track.to_ltrb()\n",
    "            bbox = ltrb\n",
    "            bottom_center = ((bbox[0] + bbox[2]) // 2, bbox[3])\n",
    "            if not above_line(bottom_center, line_y) and track_id not in d:\n",
    "                d.add(track_id)\n",
    "                current_count += 1\n",
    "            if track_id not in d1:\n",
    "                d1[track_id] = str(j)\n",
    "                j+=1\n",
    "            cv2.rectangle(img, (int(bbox[0]),int(bbox[1])),(int(bbox[2]),int(bbox[3])),(0,255,0),2)\n",
    "            cv2.putText(img, str(track_id), (int(bbox[0]),int(bbox[1]-10)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "    cv2.putText(img, f'Count: {current_count}', (20, 120), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 255), 2)\n",
    "\n",
    "    end = time.perf_counter()\n",
    "    totalTime = end - start\n",
    "    fps = 1 / totalTime\n",
    "\n",
    "    cv2.putText(img, f'FPS: {int(fps)}', (20,70), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0,255,0), 2)\n",
    "    # img = cv2.resize(img,(1800,900))\n",
    "    cv2.imshow('img', img)\n",
    "\n",
    "    # Write the frame to the output video\n",
    "    out.write(img)\n",
    "\n",
    "    k = cv2.waitKey(1)\n",
    "    if k & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(f\"Total processing time: {total_time:.2f} seconds\")\n",
    "\n",
    "# Release video capture and close all windows\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "624bd3fa-89b2-476e-8244-374d34547630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': '1', '2': '2', '3': '3', '4': '4', '5': '5', '9': '6', '10': '7', '11': '8', '14': '9', '15': '10', '18': '11', '20': '12', '22': '13'}\n"
     ]
    }
   ],
   "source": [
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a306700-b8ba-4412-a9ae-3a015b6fac5e",
   "metadata": {},
   "source": [
    "## Faster RCNN and SORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77cca3a5-09f6-4feb-944d-5f9b864c71b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total processing time: 843.53 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from sort import *\n",
    "\n",
    "# Initialize the SORT tracker\n",
    "object_tracker = Sort()\n",
    "\n",
    "# Load pre-trained Faster R-CNN model from torchvision\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Define the line near the bottom of the frame for counting cars\n",
    "line_y = 800  \n",
    "\n",
    "# Function to check if a point is above a line\n",
    "def above_line(point, line_y):\n",
    "    return point[1] < line_y\n",
    "\n",
    "# Function to perform object detection using the pre-trained model\n",
    "def detect_objects(frame):\n",
    "    # Convert frame to torch tensor\n",
    "    frame_tensor = torch.from_numpy(frame / 255.0).permute(2, 0, 1).float().unsqueeze(0)\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        predictions = model(frame_tensor)\n",
    "\n",
    "    # Process predictions\n",
    "    boxes = predictions[0]['boxes'].cpu().numpy()\n",
    "    scores = predictions[0]['scores'].cpu().numpy()\n",
    "    labels = predictions[0]['labels'].cpu().numpy()\n",
    "\n",
    "    return boxes, scores, labels\n",
    "\n",
    "# Open video capture\n",
    "cap = cv2.VideoCapture('highway.mp4')\n",
    "\n",
    "# Get frame rate of the video\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# Process every nth frame to speed up processing\n",
    "frame_skip = int(fps / 20)  # Adjust as needed\n",
    "\n",
    "frame_count = 0\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize variables for counting cars\n",
    "previous_count = 0\n",
    "current_count = 0\n",
    "j = 1\n",
    "d = set()\n",
    "d1 = {}\n",
    "sort_paths = {}\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, img = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "    if frame_count % frame_skip != 0:\n",
    "        continue\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    cv2.line(img,(80,800),(1100,800),(255,255,255),2)\n",
    "    # Perform object detection on the frame\n",
    "    boxes, scores, labels = detect_objects(img)\n",
    "\n",
    "    # Object tracking part\n",
    "    if img.any() is not None:\n",
    "        detections = [(box, score, label) for box, score, label in zip(boxes, scores, labels) if score > 0.75 and label == 3]\n",
    "\n",
    "        # Convert bounding box data to the format expected by the object tracker\n",
    "        bounding_boxes = np.array([[box[0], box[1], box[2], box[3], score] for box, score, label in detections])\n",
    "        \n",
    "        # Update tracks using the formatted bounding box data\n",
    "        tracks = object_tracker.update(bounding_boxes)\n",
    "\n",
    "        # Count cars passing through the line\n",
    "        for track in tracks:\n",
    "            bbox = track.astype(int)\n",
    "            \n",
    "            # Check if the bottom center of the bounding box crosses the line\n",
    "            bottom_center = ((bbox[0] + bbox[2]) // 2, bbox[3])\n",
    "            if not above_line(bottom_center, line_y) and track[4] not in d:\n",
    "                d.add(track[4])\n",
    "                current_count += 1\n",
    "            if track[4] not in d1:\n",
    "                d1[track[4]] = str(j)\n",
    "                j+=1\n",
    "            cv2.rectangle(img, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0, 255, 0), 2)\n",
    "            cv2.putText(img, d1[track[4]], (bbox[0], bbox[1] - 10), cv2.FONT_HERSHEY_SIMPLEX,  0.7,(0, 0, 255), 2)\n",
    "\n",
    "        # Update count\n",
    "        # if current_count > previous_count:\n",
    "        #     cars_passed = current_count - previous_count\n",
    "        #     print(f\"{cars_passed} car(s) passed through the line.\")\n",
    "        #     previous_count = current_count\n",
    "\n",
    "        # Display count on the frame\n",
    "        cv2.putText(img, f'Count: {current_count}', (20, 120), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 0), 2)\n",
    "\n",
    "    end = time.perf_counter()\n",
    "    totalTime = end - start\n",
    "    fps = 1 / totalTime\n",
    "\n",
    "    cv2.putText(img, f'FPS: {int(fps)}', (20,70), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0,255,255), 2)\n",
    "    img = cv2.resize(img,(1800,900))\n",
    "    cv2.imshow('img', img)\n",
    "\n",
    "    k = cv2.waitKey(1)\n",
    "    if k & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(f\"Total processing time: {total_time:.2f} seconds\")\n",
    "\n",
    "# Release video capture and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "732eaaaf-9c01-44ca-a48d-28cb7377cd88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(current_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7a6eb59-a02e-47cd-9ef8-54d3968bb03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total processing time: 14.26 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from sort import *\n",
    "\n",
    "# Initialize the SORT tracker\n",
    "object_tracker = Sort()\n",
    "\n",
    "# Load pre-trained Faster R-CNN model from torchvision\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Function to perform object detection using the pre-trained model\n",
    "def detect_objects(frame):\n",
    "    # Resize frame to reduce computation\n",
    "    # frame = cv2.resize(frame, (0, 0), fx=0.5, fy=0.5)\n",
    "\n",
    "    # Convert frame to torch tensor\n",
    "    frame_tensor = torch.from_numpy(frame / 255.0).permute(2, 0, 1).float().unsqueeze(0)\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        predictions = model(frame_tensor)\n",
    "\n",
    "    # Process predictions\n",
    "    boxes = predictions[0]['boxes'].cpu().numpy()\n",
    "    scores = predictions[0]['scores'].cpu().numpy()\n",
    "    labels = predictions[0]['labels'].cpu().numpy()\n",
    "\n",
    "    return boxes, scores, labels\n",
    "\n",
    "# Open video capture\n",
    "cap = cv2.VideoCapture('highway.mp4')\n",
    "\n",
    "# Get frame rate of the video\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# Process every nth frame to speed up processing\n",
    "frame_skip = int(fps / 10)  # Adjust as needed\n",
    "\n",
    "frame_count = 0\n",
    "start_time = time.time()\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, img = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "    if frame_count % frame_skip != 0:\n",
    "        continue\n",
    "\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    # Perform object detection on the frame\n",
    "    boxes, scores, labels = detect_objects(img)\n",
    "\n",
    "    # Overlay detected bounding boxes on the frame\n",
    "    # for box, score, label in zip(boxes, scores, labels):\n",
    "    #     if score > 0.5 and label == 3:  # Adjust confidence threshold and class as needed\n",
    "    #         x1, y1, x2, y2 = map(int, box)\n",
    "    #         cv2.rectangle(img, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "\n",
    "    # Object tracking part\n",
    "    if img.any() is not None:\n",
    "        detections = [(box, score, label) for box, score, label in zip(boxes, scores, labels) if score > 0.7 and label == 3]\n",
    "\n",
    "        # Convert bounding box data to the format expected by the object tracker\n",
    "        bounding_boxes = np.array([[box[0], box[1], box[2], box[3], score] for box, score, label in detections])\n",
    "        \n",
    "        # Update tracks using the formatted bounding box data\n",
    "        tracks = object_tracker.update(bounding_boxes)\n",
    "        for track in tracks:\n",
    "            # print(track)\n",
    "            bbox = track.astype(int)\n",
    "\n",
    "            cv2.rectangle(img, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0, 255, 0), 2)\n",
    "            cv2.putText(img, str(int(track[4])), (bbox[0], bbox[1] - 10), cv2.FONT_HERSHEY_SIMPLEX,  0.7,(0, 0, 255), 2)\n",
    "\n",
    "    end = time.perf_counter()\n",
    "    totalTime = end - start\n",
    "    fps = 1 / totalTime\n",
    "\n",
    "    cv2.putText(img, f'FPS: {int(fps)}', (20,70), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0,255,0), 2)\n",
    "    img = cv2.resize(img,(1800,900))\n",
    "    cv2.imshow('img', img)\n",
    "\n",
    "    k = cv2.waitKey(1)\n",
    "    if k & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(f\"Total processing time: {total_time:.2f} seconds\")\n",
    "\n",
    "# Release video capture and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddc9031-f887-4528-80f4-9fcb6c223fa4",
   "metadata": {},
   "source": [
    "## SORT vs DeepSORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4457c725-4307-4fcd-a826-83d310fa0006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total processing time: 148.86 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "from sort import Sort\n",
    "\n",
    "# Initialize the SORT tracker\n",
    "sort_tracker = Sort()\n",
    "\n",
    "# Initialize the Deep SORT tracker\n",
    "deepsort_tracker = DeepSort()\n",
    "\n",
    "# Load pre-trained Faster R-CNN model from torchvision\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Function to perform object detection using the pre-trained model\n",
    "def detect_objects(frame):\n",
    "    # Resize frame to reduce computation\n",
    "    # frame = cv2.resize(frame, (0, 0), fx=0.5, fy=0.5)\n",
    "\n",
    "    # Convert frame to torch tensor\n",
    "    frame_tensor = torch.from_numpy(frame / 255.0).permute(2, 0, 1).float().unsqueeze(0)\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        predictions = model(frame_tensor)\n",
    "\n",
    "    # Process predictions\n",
    "    boxes = predictions[0]['boxes'].cpu().numpy()\n",
    "    scores = predictions[0]['scores'].cpu().numpy()\n",
    "    labels = predictions[0]['labels'].cpu().numpy()\n",
    "\n",
    "    return boxes, scores, labels\n",
    "\n",
    "# Open video capture\n",
    "cap = cv2.VideoCapture('highway.mp4')\n",
    "\n",
    "# Get frame rate of the video\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# Process every nth frame to speed up processing\n",
    "frame_skip = int(fps / 10)  # Adjust as needed\n",
    "\n",
    "frame_count = 0\n",
    "start_time = time.time()\n",
    "\n",
    "# Dictionary to store object paths\n",
    "sort_paths = {}\n",
    "deepsort_paths = {}\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, img = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "    if frame_count % frame_skip != 0:\n",
    "        continue\n",
    "\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    # Perform object detection on the frame\n",
    "    boxes, scores, labels = detect_objects(img)\n",
    "    if img.any() is not None:\n",
    "        deepsort_detections = [(box, score, label) for box, score, label in zip(boxes, scores, labels) if score > 0.8 and label == 3]\n",
    "\n",
    "        # Convert bounding box data to the format expected by the Deep SORT tracker\n",
    "        deepsort_bounding_boxes = [([box[0], box[1], box[2] - box[0], box[3] - box[1]], score, label) for box, score, label in deepsort_detections]\n",
    "        \n",
    "        # Update tracks using the formatted bounding box data\n",
    "        deepsort_tracks = deepsort_tracker.update_tracks(deepsort_bounding_boxes, frame=img)\n",
    "        \n",
    "        # Update object paths\n",
    "        for track in deepsort_tracks:\n",
    "            if not track.is_confirmed():\n",
    "                continue\n",
    "            track_id = track.track_id\n",
    "            if track_id not in deepsort_paths:\n",
    "                deepsort_paths[track_id] = []\n",
    "            deepsort_paths[track_id].append((int((track.to_tlbr()[0] + track.to_tlbr()[2]) / 2), int((track.to_tlbr()[1] + track.to_tlbr()[3]) / 2)))\n",
    "\n",
    "            bbox = track.to_tlbr().astype(int)\n",
    "            cv2.rectangle(img, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (255, 0, 0), 2)\n",
    "            cv2.circle(img, center=(int(bbox[0]+bbox[2])//2, int(bbox[1]+bbox[3])//2), radius=3, color=(0, 0, 255), thickness=3)\n",
    "\n",
    "            cv2.putText(img, str(track_id), (bbox[0], bbox[1] - 10), cv2.FONT_HERSHEY_SIMPLEX,  0.7,(255, 0, 255), 2)\n",
    "\n",
    "        # Draw object paths\n",
    "        for track_id, path in deepsort_paths.items():\n",
    "            for i in range(1, len(path)):\n",
    "                cv2.line(img, path[i-1], path[i], (255, 0, 0), 2)\n",
    "\n",
    "    # Object tracking using SORT tracker\n",
    "    if img.any() is not None:\n",
    "        sort_detections = [(box, score, label) for box, score, label in zip(boxes, scores, labels) if score > 0.8 and label == 3]\n",
    "\n",
    "        # Convert bounding box data to the format expected by the SORT tracker\n",
    "        sort_bounding_boxes = np.array([[box[0], box[1], box[2], box[3], score] for box, score, label in sort_detections])\n",
    "        \n",
    "        # Update tracks using the formatted bounding box data\n",
    "        sort_tracks = sort_tracker.update(sort_bounding_boxes)\n",
    "        \n",
    "        # Update object paths\n",
    "        for track in sort_tracks:\n",
    "            track_id = int(track[4])\n",
    "            if track_id not in sort_paths:\n",
    "                sort_paths[track_id] = []\n",
    "            sort_paths[track_id].append((int((track[0] + track[2]) / 2), int((track[1] + track[3]) / 2)))\n",
    "\n",
    "            bbox = track.astype(int)\n",
    "            cv2.rectangle(img, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0, 255, 0), 2)\n",
    "            cv2.putText(img, str(track_id), (bbox[0], bbox[1] - 10), cv2.FONT_HERSHEY_SIMPLEX,  0.7,(0, 0, 255), 2)\n",
    "\n",
    "        # Draw object paths\n",
    "        for track_id, path in sort_paths.items():\n",
    "            for i in range(1, len(path)):\n",
    "                cv2.line(img, path[i-1], path[i], (0, 255, 0), 2)\n",
    "\n",
    "    # Object tracking using Deep SORT tracker\n",
    "    \n",
    "    end = time.perf_counter()\n",
    "    totalTime = end - start\n",
    "    fps = 1 / totalTime\n",
    "\n",
    "    cv2.putText(img, f'FPS: {int(fps)}', (20,70), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0,255,0), 2)\n",
    "    img = cv2.resize(img,(1800,900))\n",
    "    cv2.imshow('img', img)\n",
    "\n",
    "    k = cv2.waitKey(1)\n",
    "    if k & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(f\"Total processing time: {total_time:.2f} seconds\")\n",
    "\n",
    "# Release video capture and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a841621-2f0a-428a-b4a0-e0744301932f",
   "metadata": {},
   "source": [
    "## YOLOV5 + DeepSORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0270c5f6-8b64-4a30-b886-d4b31983fff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "349bf5d2-1d8e-4dad-92a0-18a44eb42457",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Necessary imports1\n",
    "import cv2\n",
    "import numpy as np\n",
    "import sys\n",
    "import glob\n",
    "\n",
    "import time\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2859cfb0-919e-4ebf-b498-820138d3788f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a class for object detection which plots boxes and scores frames in addition to detecting an \n",
    "# object\n",
    "\n",
    "class YoloDetector():\n",
    "\n",
    "    def __init__(self):\n",
    "        #Using yolov5s for our purposes of object detection, you may use a larger model\n",
    "        self.model = torch.hub.load('ultralytics/yolov5', 'yolov5m', pretrained = True)\n",
    "        self.classes = self.model.names\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        print('Using Device: ', self.device)\n",
    "    \n",
    "    def score_frame(self, frame):\n",
    "        self.model.to(self.device)\n",
    "        downscale_factor = 2\n",
    "        width = int(frame.shape[1] / downscale_factor)\n",
    "        height = int(frame.shape[0] / downscale_factor)\n",
    "        frame = cv2.resize(frame, (width, height))\n",
    "\n",
    "        results = self.model(frame)\n",
    "\n",
    "        labels, cord = results.xyxyn[0][:, -1], results.xyxyn[0][:, :-1]\n",
    "\n",
    "        return labels, cord\n",
    "    \n",
    "    def class_to_label(self, x):\n",
    "        return self.classes[int(x)]\n",
    "    \n",
    "    def plot_boxes(self, results, frame, height, width, confidence=0.7):\n",
    "        self.model.to(self.device)\n",
    "        results = self.model(frame)\n",
    "        predictions = results.xyxy[0]\n",
    "\n",
    "        detections = []\n",
    "        for pred in predictions:\n",
    "            if pred[4] >= confidence and pred[5] == 2:  # Car class index\n",
    "                x1, y1, x2, y2 = pred[:4].cpu().numpy().astype(int)\n",
    "                x_center = x1 + (x2 - x1) // 2\n",
    "                y_center = y1 + (y2 - y1) // 2\n",
    "                tlwh = np.asarray([x1, y1, x2 - x1, y2 - y1], dtype=np.float32)\n",
    "                detections.append((tlwh, float(pred[4].item()), 'car'))\n",
    "        \n",
    "        return frame, detections\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2723d117-6df3-4e6a-af04-6fbb8f4739f2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "\n",
    "# class YoloDetector():\n",
    "\n",
    "#     # def __init__(self, model_path='yolov2-voc.weights', config_path='yolov2-voc.cfg', classes_path='voc.names'):\n",
    "#     def __init__(self, model_path='yolov2.weights', config_path='yolov2.cfg', classes_path='coco.names'):\n",
    "\n",
    "#         # Load YOLOv2 model\n",
    "#         self.net = cv2.dnn.readNet(model_path, config_path)\n",
    "#         # Load class names\n",
    "#         with open(classes_path, 'r') as f:\n",
    "#             self.classes = f.read().strip().split('\\n')\n",
    "#         self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#         print('Using Device:', self.device)\n",
    "#         self.layer_names = self.net.getLayerNames()\n",
    "#         self.output_layers = [self.layer_names[i] for i in range(len(self.layer_names))]\n",
    "\n",
    "#     def score_frame(self, frame):\n",
    "#         self.net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "#         self.net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)\n",
    "\n",
    "#         blob = cv2.dnn.blobFromImage(frame, 1/255.0, (416, 416), swapRB=True, crop=False)\n",
    "#         self.net.setInput(blob)\n",
    "#         outs = self.net.forward(self.output_layers)\n",
    "\n",
    "#         h, w = frame.shape[:2]\n",
    "#         detections = []\n",
    "#         detected_objects = []\n",
    "#         for out in outs:\n",
    "#             for detection in out:\n",
    "#                 scores = detection[5:]\n",
    "#                 if len(scores) > 0:\n",
    "#                     class_id = np.argmax(scores)\n",
    "#                     if class_id < len(self.classes):\n",
    "#                         confidence = scores[class_id]\n",
    "#                         if confidence.any() > 0.2:\n",
    "#                             center_x = int(detection[0] * w)\n",
    "#                             center_y = int(detection[1] * h)\n",
    "#                             width = int(detection[2] * w)\n",
    "#                             height = int(detection[3] * h)\n",
    "#                             x1 = int(center_x - width / 2)\n",
    "#                             y1 = int(center_y - height / 2)\n",
    "#                             x2 = int(x1 + width)\n",
    "#                             y2 = int(y1 + height)\n",
    "#                             detections.append(([x1, y1, x2, y2], confidence, class_id))\n",
    "#                             detected_objects.append(class_id)\n",
    "\n",
    "#         return detected_objects, detections\n",
    "\n",
    "#     def class_to_label(self, x):\n",
    "#         return self.classes[int(x)]\n",
    "\n",
    "#     def plot_boxes(self, results, frame, height, width, confidence=0.3):\n",
    "\n",
    "#         labels, cord = results\n",
    "#         dt1 = [([10,10,1,1], 0.1, 'car')]\n",
    "\n",
    "#         n = len(labels)\n",
    "#         x_shape, y_shape = width, height\n",
    "\n",
    "#         for i in range(n):\n",
    "#             row = cord[i]\n",
    "#             print(row)\n",
    "#             if row[1]>=confidence:\n",
    "#                 x1, y1, x2, y2 = row[0][0], row[0][1], row[0][2], row[0][3]\n",
    "#                 # print(x1,y1,x2,y2)\n",
    "#                 #In this demonstration, we will only be detecting persons. You can add classes of your choice\n",
    "#                 if self.class_to_label(labels[i]) == 'car':\n",
    "\n",
    "#                     x_center = x1 + (x2-x1)\n",
    "#                     y_center = y1 + ((y2-y1) / 2)\n",
    "\n",
    "#                     tlwh = np.asarray([x1, y1, int(x2-x1), int(y2-y1)], dtype = np.float32)\n",
    "#                     confidence = float(row[1].item())\n",
    "#                     feature = 'car'\n",
    "\n",
    "#                     dt1.append(([x1, y1, int(x2-x1), int(y2-y1)], row[1].item(), 'car'))\n",
    "#         return frame,dt1\n",
    "# # Example usage:\n",
    "# detector = YoloDetector()\n",
    "# img = cv2.imread('traffic.jpg')\n",
    "# detections = detector.score_frame(img)\n",
    "# detected_frame,dt = detector.plot_boxes(detections,img,height=img.shape[0], width=img.shape[1], confidence=0.5)\n",
    "# cv2.imshow('Detected Frame', detected_frame)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22d47bac-1786-44f2-a9e5-fc8ec7233c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\opdar/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "C:\\Users\\opdar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\opdar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "YOLOv5  2024-3-16 Python-3.11.3 torch-2.2.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5m summary: 290 layers, 21172173 parameters, 0 gradients, 48.9 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device:  cpu\n"
     ]
    }
   ],
   "source": [
    "# Setting input video to webcam\n",
    "# To use your own pre-downloaded videos, write the file path instead of 0\n",
    "cap = cv2.VideoCapture(\"highway.mp4\")\n",
    "\n",
    "# Setting resolution for webcam\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "#Initializing the detection class\n",
    "detector =  YoloDetector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8c931b3-d4d0-4046-bad3-8c03ad7a174f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install deep-sort-realtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89d937f9-7228-4a88-8b59-1f12df03f011",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "\n",
    "#Initialise the object tracker class\n",
    "object_tracker = DeepSort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7fd77dd-132b-4a30-986f-1aaef1cd9ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "line_y = 800  \n",
    "\n",
    "# Function to check if a point is above a line\n",
    "def above_line(point, line_y):\n",
    "    return point[1] < line_y\n",
    "    \n",
    "w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n",
    "\n",
    "# Process every nth frame to speed up processing\n",
    "frame_skip = int(fps / 20)  # Adjust as needed\n",
    "\n",
    "# Define codec and create VideoWriter object\n",
    "out = cv2.VideoWriter(\"faster_rcnn_deep_sort.avi\",\n",
    "                       cv2.VideoWriter_fourcc(*'mp4v'),\n",
    "                       fps,\n",
    "                       (w, h))\n",
    "frame_count = 0\n",
    "d = set()\n",
    "d1 = {}\n",
    "current_count = 0\n",
    "j = 1\n",
    "while cap.isOpened():\n",
    "    success, img = cap.read()\n",
    "    if success and img.any()!=None:\n",
    "        start = time.perf_counter()\n",
    "    \n",
    "        results = detector.score_frame(img)\n",
    "        img,detections = detector.plot_boxes(results, img, height=img.shape[0], width=img.shape[1], confidence=0.85)\n",
    "        # for box,score,label in detections:\n",
    "        #     # if score > 0.3:  # Adjust confidence threshold and class as needed\n",
    "        #     cv2.rectangle(img, (int(box[0]),int(box[1])),(int(box[2]+box[0]),int(box[3]+box[1])),(0,255,255),2)\n",
    "                # print(\"here\",box)\n",
    "        cv2.line(img,(70,750),(1100,750),(255,255,255),2)\n",
    "\n",
    "        if img.any() is not None:\n",
    "            tracks = object_tracker.update_tracks(detections, frame=img) \n",
    "            # NOTE: Bounding box expects to be a list of detections, each in tuples of ([left, top, w, h], confidence, detection class)\n",
    "            \n",
    "            for track in tracks:\n",
    "                if not track.is_confirmed():\n",
    "                    continue\n",
    "                track_id = track.track_id\n",
    "                ltrb = track.to_ltrb()\n",
    "        \n",
    "                bbox = ltrb\n",
    "                # print(\"bbbox\",bbox)\n",
    "                bottom_center = ((bbox[0] + bbox[2]) // 2, bbox[3])\n",
    "\n",
    "                if not above_line(bottom_center, line_y) and track_id not in d:\n",
    "                    d.add(track_id)\n",
    "                    current_count += 1\n",
    "                if track_id not in d1:\n",
    "                    d1[track_id]=str(j)\n",
    "                    j+=1\n",
    "                cv2.rectangle(img, (int(bbox[0]),int(bbox[1])),(int(bbox[2]),int(bbox[3])),(0,0,255),2)\n",
    "                cv2.putText(img, \"ID: \" + d1[track_id], (int(bbox[0]),int(bbox[1]-10)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,0), 2)\n",
    "            cv2.putText(img, f'Count: {current_count}', (20, 120), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 255), 2)\n",
    "\n",
    "        end = time.perf_counter()\n",
    "        totalTime = end-start\n",
    "        fps = 1/totalTime\n",
    "    \n",
    "        cv2.putText(img, f'FPS: {int(fps)}', (20,70), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0,255,0), 2)\n",
    "        img = cv2.resize(img,(1800,900))\n",
    "        cv2.imshow('img', img)\n",
    "        k = cv2.waitKey(1)\n",
    "        if k & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e85d892-acb6-4017-b64b-946936ccac77",
   "metadata": {},
   "source": [
    "## YOLOv8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b1cef91-3839-4f40-912d-29371628818f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "5.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "5.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "5.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "5.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "5.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "5.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "5.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "5.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "5.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "from ultralytics.utils.checks import check_imshow\n",
    "from ultralytics.utils.plotting import Annotator, colors\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "track_history = defaultdict(lambda: [])\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "names = model.model.names\n",
    "\n",
    "video_path = \"highway.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "assert cap.isOpened(), \"Error reading video file\"\n",
    "\n",
    "w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if success:\n",
    "        results = model.track(frame, persist=True, verbose=False)\n",
    "        boxes = results[0].boxes.xyxy.cpu()\n",
    "\n",
    "        if results[0].boxes.id is not None:\n",
    "            # print(results)\n",
    "            # Extract prediction results\n",
    "            clss = results[0].boxes.cls.cpu().tolist()\n",
    "            track_ids = results[0].boxes.id.int().cpu().tolist()\n",
    "            confs = results[0].boxes.conf.float().cpu().tolist()\n",
    "\n",
    "            # Annotator Init\n",
    "            annotator = Annotator(frame, line_width=2)\n",
    "\n",
    "            for box, cls, track_id in zip(boxes, clss, track_ids):\n",
    "                print(cls)\n",
    "                if cls==2.0:\n",
    "                    annotator.box_label(box, color=colors(int(cls), True), label=names[int(cls)])\n",
    "                    # Store tracking history\n",
    "                    track = track_history[track_id]\n",
    "                    track.append((int((box[0] + box[2]) / 2), int((box[1] + box[3]) / 2)))\n",
    "                    if len(track) > 30:\n",
    "                        track.pop(0)\n",
    "    \n",
    "                    # Plot tracks\n",
    "                    points = np.array(track, dtype=np.int32).reshape((-1, 1, 2))\n",
    "                    cv2.circle(frame, (track[-1]), 7, colors(int(cls), True), -1)\n",
    "                    cv2.polylines(frame, [points], isClosed=False, color=colors(int(cls), True), thickness=2)\n",
    "        frame = cv2.resize(frame,(1800,900))\n",
    "\n",
    "        cv2.imshow(\"Object Tracking\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "042f73b5-7c48-4cce-b5ff-845a3442506c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from ultralytics.utils.plotting import Annotator, colors\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "track_history = defaultdict(lambda: [])\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "names = model.model.names\n",
    "\n",
    "video_path = \"highway.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "assert cap.isOpened(), \"Error reading video file\"\n",
    "\n",
    "w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n",
    "\n",
    "frame_count = 0\n",
    "start_time = time.time()\n",
    "fps_text = \"\"\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if success:\n",
    "        frame_count += 1\n",
    "        results = model.track(frame, persist=True, verbose=False)\n",
    "        boxes = results[0].boxes.xyxy.cpu()\n",
    "\n",
    "        if results[0].boxes.id is not None:\n",
    "            clss = results[0].boxes.cls.cpu().tolist()\n",
    "            track_ids = results[0].boxes.id.int().cpu().tolist()\n",
    "            confs = results[0].boxes.conf.float().cpu().tolist()\n",
    "\n",
    "            annotator = Annotator(frame, line_width=2)\n",
    "\n",
    "            for box, cls, track_id, conf in zip(boxes, clss, track_ids, confs):\n",
    "                if cls == 2.0 and conf>0.5:\n",
    "                    annotator.box_label(box, color=colors(int(cls), True), label=f\"{names[int(cls)]} ({conf:.2f})\")\n",
    "                    track = track_history[track_id]\n",
    "                    track.append((int((box[0] + box[2]) / 2), int((box[1] + box[3]) / 2)))\n",
    "                    if len(track) > 30:\n",
    "                        track.pop(0)\n",
    "\n",
    "                    points = np.array(track, dtype=np.int32).reshape((-1, 1, 2))\n",
    "                    cv2.circle(frame, (track[-1]), 7, colors(int(cls), True), -1)\n",
    "                    cv2.polylines(frame, [points], isClosed=False, color=colors(int(cls), True), thickness=2)\n",
    "        \n",
    "        # Calculate FPS\n",
    "        if frame_count % 10 == 0:  # Update FPS every 10 frames\n",
    "            end_time = time.time()\n",
    "            fps = frame_count / (end_time - start_time)\n",
    "            fps_text = f\"FPS: {round(fps, 2)}\"\n",
    "            frame_count = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "        # Display FPS on frame\n",
    "        cv2.putText(frame, fps_text, (30, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "        frame = cv2.resize(frame, (1800, 900))\n",
    "        cv2.imshow(\"Object Tracking\", frame)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27002ba4-f192-42f4-a640-a61c38855c37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
